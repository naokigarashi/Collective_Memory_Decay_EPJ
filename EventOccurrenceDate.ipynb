{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get event occurrence date for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%get time:2022-03-23 14:00:12.500964\n",
      "number of URL data: 150data\n",
      "can get date:74data\n"
     ]
    }
   ],
   "source": [
    "disaster_lis=[]#name of earthquake(list)\n",
    "url=\"https://en.wikipedia.org/wiki/List_of_earthquakes_2011-2020\"#summary page\n",
    "html = request.urlopen(url)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "td_tag = soup.find_all(\"td\")#all link tag\n",
    "pattern1 = r'<i>see</i> <a (.*)</a>'\n",
    "pattern2 = r'href=\"\\/wiki\\/([^ ]+)\"'\n",
    "\n",
    "for tag in td_tag:\n",
    "    result1 = re.findall(pattern1, str(tag))\n",
    "    if result1 != []:\n",
    "        result2 = re.findall(pattern2, result1[0])\n",
    "        disaster_lis.append(result2[0])\n",
    "\n",
    "\n",
    "pattern = r'<td class=\"infobox-data\">(\\d*-\\d*-\\d*) \\d*:\\d*:\\d*</td>'\n",
    "disaster_dict={}#dictionary(name of earthquake:event occurance date)\n",
    "not_get=[]\n",
    "count=0\n",
    "\n",
    "for disaster in list(dict.fromkeys(disaster_lis)):#Avoid duplication and repeat\n",
    "\n",
    "    url=\"https://en.wikipedia.org/wiki/{0}\".format(disaster)\n",
    "    html = request.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    info = soup.find(class_=\"infobox-data\")\n",
    "    result = re.findall(pattern, str(info))\n",
    "    \n",
    "    if result == []:\n",
    "        not_get.append(disaster)\n",
    "    else:\n",
    "        event_date=datetime.datetime.strptime(result[0], '%Y-%m-%d')#Event Occurence Date\n",
    "        from_date = datetime.datetime(2015, 7, 1)\n",
    "        to_date = datetime.datetime(2020, 7, 1)\n",
    "\n",
    "        if from_date <= event_date < to_date: \n",
    "            disaster_dict[disaster]=event_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "        else:\n",
    "            not_get.append(disaster)\n",
    "    count+=1\n",
    "    print(f\"\\r{count*100/len(list(dict.fromkeys(disaster_lis)))}%\", end='')\n",
    "    \n",
    "dt_now = datetime.datetime.now()\n",
    "print(f\"get time:{dt_now}\")\n",
    "print(f\"number of URL data: {len(list(dict.fromkeys(disaster_lis)))}data\")\n",
    "print(f\"can get date:{len(disaster_dict)}data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save\n",
    "with open(\"NameURL/disaster.pkl\",\"wb\") as f:\n",
    "    pickle.dump(disaster_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tero incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=[2015,2016,2017,2018,2019,2020]#year list\n",
    "tero_dict={}#dictionary(name of tero:event occurance date)\n",
    "page=0\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    url=\"https://en.wikipedia.org/wiki/List_of_terrorist_incidents_in_\"+str(year)#summary page\n",
    "    data = pd.read_html(url, header = 0)\n",
    "\n",
    "    tero_name_lis=data[0][\"Article\"].values.tolist()#get the list of tero from the table\n",
    "    tero_lis=[]#list of URL\n",
    "    pattern = r'href=\"\\/wiki\\/([^ ]+)\"'\n",
    "    html = request.urlopen(url)\n",
    "    soup = BeautifulSoup(html,\"html.parser\",from_encoding='utf-8')\n",
    "\n",
    "    for name in tero_name_lis:\n",
    "\n",
    "        title_tero=soup.find_all(title=name)\n",
    "        result = re.findall(pattern, str(title_tero))\n",
    "\n",
    "        if result!=[]:\n",
    "            tero_lis.append(result[0])\n",
    "        else:\n",
    "            tero_lis.append(np.nan)\n",
    "\n",
    "    data[page][\"wikiURL\"]=tero_lis#newly added to data frame\n",
    "\n",
    "    #Deletes dates other than one specific day(Dec 19-23)\n",
    "    df1=data[page][[\"Date\",\"wikiURL\"]][~data[page]['Date'].str.contains('-')&~data[page]['Date'].str.contains('â€“')&~data[page]['Date'].str.contains('and')]\n",
    "    \n",
    "    #Get date information\n",
    "    date_lis=[]\n",
    "    for date in df1[\"Date\"].values.tolist():#\n",
    "\n",
    "        #Since the notation differs depending on the year, obtain it by changing the conditions\n",
    "        if year==2016 or year==2017:\n",
    "            date_new_time=datetime.datetime.strptime(date,\"%b %d\")\n",
    "        elif year==2015:\n",
    "            pattern='^[0-9]'\n",
    "            result = re.findall(pattern, date)\n",
    "            if result==[]:#when starting with a number, how to take the date first\n",
    "                date_new_time=datetime.datetime.strptime(date,\"%b %d\")\n",
    "            else:\n",
    "                date_new_time=datetime.datetime.strptime(date,\"%d %b\")\n",
    "        else:    \n",
    "            date_new_time=datetime.datetime.strptime(date,\"%d %B\")\n",
    "\n",
    "        date_new=date_new_time.strftime(str(year)+'-%m-%d')\n",
    "        date_lis.append(date_new)\n",
    "\n",
    "    #Get a dictionary (URL: event occurrence date) for a limited time\n",
    "    for URL,date in zip(df1[\"wikiURL\"].values.tolist(),date_lis):\n",
    "\n",
    "        event_date=datetime.datetime.strptime(date, '%Y-%m-%d')#event occurrence date\n",
    "        from_date = datetime.datetime(2015, 7, 1)\n",
    "        to_date = datetime.datetime(2020, 7, 1)\n",
    "\n",
    "        if from_date <= event_date < to_date: \n",
    "                tero_dict[URL]=event_date.strftime('%Y-%m-%d')\n",
    "\n",
    "dt_now = datetime.datetime.now()\n",
    "print(f\"get time:{dt_now}\")\n",
    "print(f\"can get date:{len(tero_dict)}data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "with open(\"NameURL/tero.pkl\",\"wb\") as f:\n",
    "    pickle.dump(tero_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notable death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015,2016,2017,2018,2019,2020]\n",
    "months =[\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "\n",
    "pattern = r'^<li><a href=\"\\/wiki\\/[^ ]+\"'\n",
    "death_lis=[]#List of all people\n",
    "for year in years:\n",
    "    \n",
    "    for month in months:\n",
    "        url = \"https://en.wikipedia.org/wiki/Deaths_in_{0}_{1}\".format(month,str(year))\n",
    "        html = request.urlopen(url)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        li_tag = soup.find_all(\"li\")#get all link tags\n",
    "\n",
    "        for i in range(len(li_tag)):\n",
    "            result = re.findall(pattern, str(li_tag[i]))\n",
    "            if result != []:\n",
    "                name = result[0][19:-1]\n",
    "                if \"Deaths_in\" not in name and \"Category\" not in name:\n",
    "                    death_lis.append(result[0][19:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the death date from each Wikipedia page and make it a dictionary type\n",
    "death_dict={}\n",
    "pattern = r'<span style=\"display:none\">\\((\\d*-\\d*-\\d*)\\)</span>'\n",
    "kara_lis=[]\n",
    "zerozero_lis=[]\n",
    "count=0\n",
    "\n",
    "for death in death_lis:\n",
    "\n",
    "    url=\"https://en.wikipedia.org/wiki/{0}\".format(death)\n",
    "    html = request.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    infos = soup.find_all(class_=\"infobox-data\")#get all infobox-data tag\n",
    "\n",
    "    for info in infos:\n",
    "        result = re.findall(pattern, str(info))\n",
    "\n",
    "        if result!=[] and result[0][-2:]!=\"00\":\n",
    "            event_date=datetime.datetime.strptime(result[0], '%Y-%m-%d')#event occurance date\n",
    "            from_date = datetime.datetime(2015, 7, 1)\n",
    "            to_date = datetime.datetime(2020, 7, 1)\n",
    "\n",
    "            if from_date <= event_date < to_date: \n",
    "                        death_dict[death]=event_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    count+=1\n",
    "    print(f\"\\r{count*100/len(death_lis)}%\", end='')\n",
    "\n",
    "#death_dict\n",
    "print(f\"get time:{dt_now}\")\n",
    "print(f\"can get date:{len(death_dict)}data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "with open(\"NameURL/death.pkl\",\"wb\") as f:\n",
    "    pickle.dump(death_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aviation accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=[2015,2016,2017,2018,2019,2020]\n",
    "\n",
    "aviation_dict={}\n",
    "for year in years:\n",
    "    url=\"https://en.wikipedia.org/wiki/Category:Aviation_accidents_and_incidents_in_\"+str(year)#summary page\n",
    "    html = request.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    li_tag = soup.find_all(\"li\")#get all link tag\n",
    "    pattern = r'^<li><a href=\"\\/wiki\\/[^ ]+\"'\n",
    "    aviation_lis=[]\n",
    "\n",
    "    for i in range(len(li_tag)):\n",
    "        result = re.findall(pattern, str(li_tag[i]))\n",
    "        if result != []:\n",
    "            name = result[0][19:-1]\n",
    "            if \"Category\" not in name:\n",
    "                aviation_lis.append(result[0][19:-1])\n",
    "\n",
    "    for aviation in aviation_lis:\n",
    "        url_each=\"https://en.wikipedia.org/wiki/\"+aviation#summary page\n",
    "        html = request.urlopen(url_each)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        #get event occurance date\n",
    "        span = soup.find('span', class_='bday dtstart published updated')\n",
    "        date_pattern=r'^<span class=\"bday dtstart published updated\">([0-9]+-[0-9]+-[0-9]+)</span>'\n",
    "        result = re.findall(date_pattern, str(span))\n",
    "        if result!=[]:\n",
    "            event_date=datetime.datetime.strptime(result[0], '%Y-%m-%d')#event occurance date\n",
    "            from_date = datetime.datetime(2015, 7, 1)\n",
    "            to_date = datetime.datetime(2020, 7, 1)\n",
    "\n",
    "            if from_date <= event_date < to_date: \n",
    "                        aviation_dict[aviation]=event_date.strftime('%Y-%m-%d')\n",
    "dt_now = datetime.datetime.now()\n",
    "print(f\"get time:{dt_now}\")\n",
    "print(f\"can get date:{len(aviation_dict)}data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"NameURL/aviation.pkl\",\"wb\") as f:\n",
    "    pickle.dump(aviation_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass murder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=[2015,2016,2017,2018,2019,2020]\n",
    "murder_dict={}\n",
    "for year in years:\n",
    "    url=\"https://en.wikipedia.org/wiki/Category:Mass_murder_in_\"+str(year)\n",
    "    html = request.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    li_tag = soup.find_all(\"li\")#get all link tag\n",
    "    pattern = r'^<li><a href=\"\\/wiki\\/[^ ]+\"'\n",
    "    murder_lis=[]\n",
    "\n",
    "    for i in range(len(li_tag)):\n",
    "        result = re.findall(pattern, str(li_tag[i]))\n",
    "        if result != []:\n",
    "            name = result[0][19:-1]\n",
    "            if \"Category\" not in name:\n",
    "                murder_lis.append(result[0][19:-1])\n",
    "\n",
    "    for murder in murder_lis:\n",
    "        url_each=\"https://en.wikipedia.org/wiki/\"+murder#summary page\n",
    "        html = request.urlopen(url_each)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        ##get event occurance date\n",
    "        span = soup.find('span', class_='bday dtstart published updated')\n",
    "        date_pattern=r'^<span class=\"bday dtstart published updated\">([0-9]+-[0-9]+-[0-9]+)</span>'\n",
    "        result = re.findall(date_pattern, str(span))\n",
    "        if result!=[]:\n",
    "            event_date=datetime.datetime.strptime(result[0], '%Y-%m-%d')#event occurance date\n",
    "            from_date = datetime.datetime(2015, 7, 1)\n",
    "            to_date = datetime.datetime(2020, 7, 1)\n",
    "\n",
    "            if from_date <= event_date < to_date: \n",
    "                        murder_dict[murder]=event_date.strftime('%Y-%m-%d')\n",
    "                    \n",
    "dt_now = datetime.datetime.now()\n",
    "print(f\"get time:{dt_now}\")\n",
    "print(f\"can get date:{len(murder_dict)}data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"NameURL/murder.pkl\",\"wb\") as f:\n",
    "    pickle.dump(murder_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
