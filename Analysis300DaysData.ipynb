{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as optimize\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import scipy.stats\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read URL data\n",
    "with open('NameURL/disaster.pkl', 'rb') as f:\n",
    "    disaster_dict = pickle.load(f)\n",
    "\n",
    "with open('NameURL/death.pkl', 'rb') as f:\n",
    "    death_dict = pickle.load(f)\n",
    "\n",
    "with open('NameURL/aviation.pkl', 'rb') as f:\n",
    "    aviation_dict = pickle.load(f)\n",
    "\n",
    "with open('NameURL/murder.pkl', 'rb') as f:\n",
    "    murder_dict = pickle.load(f)\n",
    "\n",
    "with open('NameURL/tero.pkl', 'rb') as f:\n",
    "    tero_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of pages that have been dated\n",
    "files_dict=[disaster_dict, death_dict, aviation_dict, murder_dict, tero_dict]\n",
    "\n",
    "for file in files_dict:\n",
    "    print(f\"{len(file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the number of views for 300 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pageviewapi\n",
    "from datetime import datetime as dt\n",
    "import requests\n",
    "from attrdict import AttrDict\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "__version__   = \"0.4.0\"\n",
    "PROJECT_URL   = \"https://github.com/Commonists/pageview-api\"\n",
    "UA            = \"Python pageview-api client v{version} <{url}>\"\n",
    "USER_AGENT    = {'User-Agent': UA.format(url=PROJECT_URL, version=__version__)}\n",
    "API_BASE_URL  = \"https://wikimedia.org/api/rest_v1/metrics\"\n",
    "PA_ENDPOINT   = \"pageviews/per-article\"\n",
    "PA_ARGS       = \"{project}/{access}/{agent}/{page}/{granularity}/{start}/{end}\"\n",
    "\n",
    "\n",
    "def per_article(project, page, start, end,access='all-access', agent='all-agents', granularity='daily'):\n",
    "\n",
    "    args = PA_ARGS.format(project=project, page=page, start=start, end=end, access=access, agent=agent, granularity=granularity)\n",
    "    return __api__(PA_ENDPOINT, args)\n",
    "\n",
    "def __api__(end_point, args, api_url=API_BASE_URL):\n",
    "    \n",
    "    url = \"/\".join([api_url, end_point, args])\n",
    "    response = requests.get(url, headers=USER_AGENT)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    \n",
    "    elif response.status_code == 404:\n",
    "        pass\n",
    "        #print(\"error404:  \"+args)\n",
    "    \n",
    "    #elif response.status_code == 429:\n",
    "        #raise ThrottlingException\n",
    "    \n",
    "    else:\n",
    "        response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_lis=[disaster_dict,aviation_dict,murder_dict,tero_dict,death_dict]\n",
    "file_names=[\"disaster\",\"aviation\",\"murder\",\"tero\",\"death\"]\n",
    "\n",
    "for name_dict,file in zip(analysis_lis,file_names):\n",
    "    \n",
    "    #Create an empty time series data frame (index is 0 ~ 299)\n",
    "    date_list =list(range(0, 300, 1))\n",
    "    df_decay=pd.DataFrame(index=date_list)\n",
    "\n",
    "    for name in name_dict:\n",
    "        page_view_lis=[]\n",
    "        start_date=datetime.datetime.strptime(name_dict[name], '%Y-%m-%d')#event occurance date\n",
    "        last_date=start_date+datetime.timedelta(days=299)\n",
    "\n",
    "        response=per_article('en.wikipedia', name, start_date.strftime('%Y%m%d'), last_date.strftime('%Y%m%d'), access='all-access', agent='user', granularity='daily')\n",
    "\n",
    "        if type(response)==dict:\n",
    "\n",
    "            Wikipedia_start=datetime.datetime.strptime(response['items'][0][\"timestamp\"], '%Y%m%d00')#The first day when the number of Wikipedia pages was obtained\n",
    "            diff_date=Wikipedia_start-start_date\n",
    "\n",
    "            if diff_date.days>0:#If there is nan until the first day, enter it\n",
    "                page_view_lis=[np.nan]*diff_date.days\n",
    "                \n",
    "            if len(page_view_lis)<7 and len(page_view_lis)+len(response['items'])==300:\n",
    "                \n",
    "                for i in range(len(response['items'])):\n",
    "                        page_view_lis.append(response['items'][i][\"views\"])#Add the number of views to the list in order\n",
    "                df_decay[name]=page_view_lis\n",
    "\n",
    "    dt_now = datetime.datetime.now()\n",
    "    print(f\"get time({file}):{dt_now}\")\n",
    "\n",
    "    df_decay.to_csv(f\"TimeSeries/{file}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of function to approximate\n",
    "def beki(x, A, a):\n",
    "    return np.log10( A * (x**(-a)) )\n",
    "\n",
    "def exp(x, A, a):\n",
    "    return np.log10( A * np.exp(-a*x))\n",
    "\n",
    "def biexp(x, A, a, B, b):\n",
    "    return np.log10( A * np.exp(-a*x) + B * np.exp(-b*x) )\n",
    "\n",
    "def stretch(x, a, b, c):\n",
    "    return np.log10( np.exp( -(x/a)**( b*x**(-c)) ) )\n",
    "\n",
    "def shiftbeki(x, a, b, c):\n",
    "    return np.log10( a * (x**(-b)) + c )\n",
    "\n",
    "def expbeki(x, A, a, B, b):\n",
    "    return np.log10( A *np.exp(-a*x)+ B*(x**(-b)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "file_names=[\"disaster\",\"aviation\",\"murder\",\"tero\",\"death\"]\n",
    "\n",
    "for file in file_names:\n",
    "    df_decay=pd.read_csv(f\"TimeSeries/{file}.csv\", index_col=0)\n",
    "\n",
    "    for name in df_decay:\n",
    "        df_peak=df_decay[name][df_decay[name].idxmax():]#Change to a curve from the peak\n",
    "        df_norm=df_peak/df_peak.max()#Normalization\n",
    "        \n",
    "        if  df_peak.max()<1000 or df_decay[name].idxmax()>=5:\n",
    "            df_decay=df_decay.drop(name, axis=1)\n",
    "            \n",
    "    print(f\"{file}_after:{len(df_decay.columns)}\")\n",
    "    df_decay.to_csv(f\"TimeSeries_pre/{file}_pre.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=[\"disaster\",\"aviation\",\"murder\",\"tero\",\"death\"]\n",
    "\n",
    "for file in file_names:\n",
    "    df_decay=pd.read_csv(f\"TimeSeries_pre/{file}_pre.csv\", index_col=0)\n",
    "    \n",
    "    #Create R^2 data frame (index is function name)\n",
    "    r_list =[\"day_to_peak\",\"peak\",\"beki\",\"exp\",\"biexp\",\"stretch\",\"shiftbeki\",\"expbeki\",\"expbeki_A\",\"expbeki_a\",\"expbeki_B\",\"expbeki_b\",\"switching_point\"]\n",
    "    df_r=pd.DataFrame(index=r_list)\n",
    "    \n",
    "    #Create AIC data frame (index is function name)\n",
    "    aic_list =[\"beki\",\"exp\",\"biexp\",\"stretch\",\"shiftbeki\",\"expbeki\"]\n",
    "    df_aic=pd.DataFrame(index=aic_list)\n",
    "    \n",
    "    for name in df_decay:\n",
    "        \n",
    "        df_fromax=df_decay[name][df_decay[name].idxmax():]#Change to a curve from the peak\n",
    "        minimum = np.min(df_fromax.values[np.nonzero(df_fromax.values)])#Get the minimum value other than 0\n",
    "        df=df_fromax+minimum#Curve from peak + minimum value\n",
    "        df_norm=df/df.max()#Normalization\n",
    "        df_norm.reset_index(drop=True, inplace=True)#Index 0 start\n",
    "\n",
    "        #analysis target\n",
    "        y_analysis=np.log10(df_norm)#Normalized → logarithmically transformed time series data\n",
    "        x_analysis = np.linspace(1,len(y_analysis),len(y_analysis))#x\n",
    "        \n",
    "        \n",
    "        # Find the approximation function\n",
    "        popt_beki, _      = optimize.curve_fit(beki, x_analysis, y_analysis, bounds=((0,0),(np.inf,np.inf)) )\n",
    "        popt_exp, _       = optimize.curve_fit(exp, x_analysis, y_analysis, bounds=((0,0),(np.inf,np.inf)) )\n",
    "        popt_biexp, _     = optimize.curve_fit(biexp, x_analysis, y_analysis, bounds=((0,0,0,0),(np.inf,np.inf,np.inf,np.inf)))\n",
    "        popt_stretch, _   = optimize.curve_fit(stretch, x_analysis, y_analysis, bounds=((0,0,0),(np.inf,np.inf,np.inf)))\n",
    "        popt_shiftbeki, _ = optimize.curve_fit(shiftbeki, x_analysis, y_analysis, bounds=((0,0,0),(np.inf,np.inf,np.inf)))\n",
    "        popt_expbeki, _ = optimize.curve_fit(expbeki, x_analysis, y_analysis, bounds=((0,0,0,0),(np.inf,np.inf,np.inf,np.inf)))\n",
    "\n",
    "        \n",
    "        #Find the coefficient of determination\n",
    "        r_beki     = metrics.r2_score(y_analysis, beki(x_analysis, *popt_beki))\n",
    "        r_exp      = metrics.r2_score(y_analysis, exp(x_analysis, *popt_exp))\n",
    "        r_biexp    = metrics.r2_score(y_analysis, biexp(x_analysis, *popt_biexp))\n",
    "        r_stretch  = metrics.r2_score(y_analysis, stretch(x_analysis, *popt_stretch))\n",
    "        r_shiftbeki= metrics.r2_score(y_analysis, shiftbeki(x_analysis, *popt_shiftbeki))\n",
    "        r_expbeki  = metrics.r2_score(y_analysis, expbeki(x_analysis, *popt_expbeki))\n",
    "\n",
    "        \n",
    "        #MemorySwitchingPoint\n",
    "        diff=exp(x_analysis,popt_expbeki[0],popt_expbeki[1])-beki(x_analysis,popt_expbeki[2],popt_expbeki[3])\n",
    "\n",
    "        index=1\n",
    "        switching_point=365#Permanent index (parameter without turning point)\n",
    "        for i in diff:\n",
    "            if i<0:\n",
    "                switching_point=index\n",
    "                break\n",
    "            index+=1\n",
    "\n",
    "        df_r[name]=[df_decay[name].idxmax(),df_fromax.max(),r_beki,r_exp,r_biexp,r_stretch,r_shiftbeki,r_expbeki\n",
    "                            ,popt_expbeki[0],popt_expbeki[1],popt_expbeki[2],popt_expbeki[3],switching_point]\n",
    "\n",
    "\n",
    "        #AIC\n",
    "        result_beki = sm.OLS(y_analysis,beki(x_analysis,popt_beki[0],popt_beki[1])).fit()\n",
    "        result_exp = sm.OLS(y_analysis,exp(x_analysis,popt_exp[0],popt_exp[1])).fit()\n",
    "        result_biexp = sm.OLS(y_analysis,biexp(x_analysis,popt_biexp[0],popt_biexp[1],popt_biexp[2],popt_biexp[3])).fit()\n",
    "        result_stretch = sm.OLS(y_analysis,stretch(x_analysis,popt_stretch[0],popt_stretch[1],popt_stretch[2])).fit()\n",
    "        result_shiftbeki = sm.OLS(y_analysis,shiftbeki(x_analysis,popt_shiftbeki[0],popt_shiftbeki[1],popt_shiftbeki[2])).fit()\n",
    "        result_expbeki = sm.OLS(y_analysis,expbeki(x_analysis,popt_expbeki[0],popt_expbeki[1],popt_expbeki[2],popt_expbeki[3])).fit()\n",
    "\n",
    "        df_aic[name]=[result_beki.aic,result_exp.aic,result_biexp.aic,result_stretch.aic,result_shiftbeki.aic,result_expbeki.aic]\n",
    "        \n",
    "        df_r.to_csv(f\"parameter/{file}_parameter_r.csv\")\n",
    "        df_aic.to_csv(f\"parameter/{file}_parameter_aic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disaster_r=pd.read_csv(f\"parameter/disaster_parameter_r.csv\",index_col=0)\n",
    "df_aviation_r=pd.read_csv(f\"parameter/aviation_parameter_r.csv\",index_col=0)\n",
    "df_murder_r=pd.read_csv(f\"parameter/murder_parameter_r.csv\",index_col=0)\n",
    "df_tero_r=pd.read_csv(f\"parameter/tero_parameter_r.csv\",index_col=0)\n",
    "df_death_r=pd.read_csv(f\"parameter/death_parameter_r.csv\",index_col=0)\n",
    "\n",
    "df_disaster_aic=pd.read_csv(f\"parameter/disaster_parameter_aic.csv\",index_col=0)\n",
    "df_aviation_aic=pd.read_csv(f\"parameter/aviation_parameter_aic.csv\",index_col=0)\n",
    "df_murder_aic=pd.read_csv(f\"parameter/murder_parameter_aic.csv\",index_col=0)\n",
    "df_tero_aic=pd.read_csv(f\"parameter/tero_parameter_aic.csv\",index_col=0)\n",
    "df_death_aic=pd.read_csv(f\"parameter/death_parameter_aic.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter evaluation obtained by fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[round(df_disaster_r.loc[\"beki\"].median(),3),round(df_death_r.loc[\"beki\"].median(),3),round(df_aviation_r.loc[\"beki\"].median(),3),round(df_murder_r.loc[\"beki\"].median(),3),round(df_tero_r.loc[\"beki\"].median(),3)],\n",
    "                 [round(df_disaster_r.loc[\"exp\"].median(),3),round(df_death_r.loc[\"exp\"].median(),3),round(df_aviation_r.loc[\"exp\"].median(),3),round(df_murder_r.loc[\"exp\"].median(),3),round(df_tero_r.loc[\"exp\"].median(),3)],\n",
    "                 [round(df_disaster_r.loc[\"biexp\"].median(),3),round(df_death_r.loc[\"biexp\"].median(),3),round(df_aviation_r.loc[\"biexp\"].median(),3),round(df_murder_r.loc[\"biexp\"].median(),3),round(df_tero_r.loc[\"biexp\"].median(),3)],\n",
    "                 [round(df_disaster_r.loc[\"stretch\"].median(),3),round(df_death_r.loc[\"stretch\"].median(),3),round(df_aviation_r.loc[\"stretch\"].median(),3),round(df_murder_r.loc[\"stretch\"].median(),3),round(df_tero_r.loc[\"stretch\"].median(),3)],\n",
    "                 [round(df_disaster_r.loc[\"shiftbeki\"].median(),3),round(df_death_r.loc[\"shiftbeki\"].median(),3),round(df_aviation_r.loc[\"shiftbeki\"].median(),3),round(df_murder_r.loc[\"shiftbeki\"].median(),3),round(df_tero_r.loc[\"shiftbeki\"].median(),3)],\n",
    "                 [round(df_disaster_r.loc[\"expbeki\"].median(),3),round(df_death_r.loc[\"expbeki\"].median(),3),round(df_aviation_r.loc[\"expbeki\"].median(),3),round(df_murder_r.loc[\"expbeki\"].median(),3),round(df_tero_r.loc[\"expbeki\"].median(),3)]])\n",
    "\n",
    "axis1 = np.array([\"earthquake\",\"death\",\"aviation\",\"murder\",\"tero\"])\n",
    "axis2 = [\"beki\",\"exp\",\"biexp\",\"stretch\",\"shift\",\"exp-power\"]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.axis('tight')\n",
    "plt.axis('off')\n",
    "plt.table(cellText=data, colLabels=axis1, rowLabels=axis2, loc=\"center\",fontsize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notable death\n",
    "fig=plt.figure()\n",
    "\n",
    "death_lis=df_death_r.loc[\"expbeki\"].values.tolist()\n",
    "death_weight=np.ones_like(death_lis)/len(death_lis)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "bins_lis=[]\n",
    "for i in list(range(0,51,1)):\n",
    "    bins_lis.append(i/50)\n",
    "\n",
    "plt.hist(death_lis,bins=bins_lis,weights=death_weight,alpha=1)\n",
    "plt.xlabel(\"$R^{2}$\",size=16)\n",
    "plt.ylabel(\"Probablity\",size=16)\n",
    "#plt.savefig(f\"C:/Users/naoki/OneDrive/デスクトップ/figure_work/hist_r.eps\")\n",
    "plt.show()\n",
    "statistics.median(death_lis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[round(df_disaster_aic.loc[\"beki\"].median(),1),round(df_death_aic.loc[\"beki\"].median(),1),round(df_aviation_aic.loc[\"beki\"].median(),1),round(df_murder_aic.loc[\"beki\"].median(),1),round(df_tero_aic.loc[\"beki\"].median(),1)],\n",
    "                 [round(df_disaster_aic.loc[\"exp\"].median(),1),round(df_death_aic.loc[\"exp\"].median(),1),round(df_aviation_aic.loc[\"exp\"].median(),1),round(df_murder_aic.loc[\"exp\"].median(),1),round(df_tero_aic.loc[\"exp\"].median(),1)],\n",
    "                 [round(df_disaster_aic.loc[\"biexp\"].median(),1),round(df_death_aic.loc[\"biexp\"].median(),1),round(df_aviation_aic.loc[\"biexp\"].median(),1),round(df_murder_aic.loc[\"biexp\"].median(),1),round(df_tero_aic.loc[\"biexp\"].median(),1)],\n",
    "                 [round(df_disaster_aic.loc[\"stretch\"].median(),1),round(df_death_aic.loc[\"stretch\"].median(),1),round(df_aviation_aic.loc[\"stretch\"].median(),1),round(df_murder_aic.loc[\"stretch\"].median(),1),round(df_tero_aic.loc[\"stretch\"].median(),1)],\n",
    "                 [round(df_disaster_aic.loc[\"shiftbeki\"].median(),1),round(df_death_aic.loc[\"shiftbeki\"].median(),1),round(df_aviation_aic.loc[\"shiftbeki\"].median(),1),round(df_murder_aic.loc[\"shiftbeki\"].median(),1),round(df_tero_aic.loc[\"shiftbeki\"].median(),1)],\n",
    "                 [round(df_disaster_aic.loc[\"expbeki\"].median(),1),round(df_death_aic.loc[\"expbeki\"].median(),1),round(df_aviation_aic.loc[\"expbeki\"].median(),1),round(df_murder_aic.loc[\"expbeki\"].median(),1),round(df_tero_aic.loc[\"expbeki\"].median(),1)]])\n",
    "\n",
    "axis1 = np.array([\"earthquake\",\"death\",\"aviation\",\"murder\",\"tero\"])\n",
    "axis2 = [\"beki\",\"exp\",\"biexp\",\"stretch\",\"shift\",\"exp-power\"]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.axis('tight')\n",
    "plt.axis('off')\n",
    "plt.table(cellText=data, colLabels=axis1, rowLabels=axis2, loc=\"center\",fontsize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notable death\n",
    "fig=plt.figure()\n",
    "\n",
    "death_lis=df_death_aic.loc[\"expbeki\"].values.tolist()\n",
    "death_weight=np.ones_like(death_lis)/len(death_lis)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "bins_lis=[]\n",
    "for i in list(range(-15,15,1)):\n",
    "    bins_lis.append(100*i)\n",
    "\n",
    "plt.hist(death_lis,bins=bins_lis,weights=death_weight,alpha=1)\n",
    "plt.xlabel(\"$AIC$\",size=16)\n",
    "plt.ylabel(\"Probablity\",size=16)\n",
    "#fig.savefig(f\"C:/Users/naoki/OneDrive/デスクトップ/figure_work/deathhistaic.eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter evaluation of exponent + power (alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#β\n",
    "\n",
    "bins_lis=[]\n",
    "for i in list(range(0,40,1)):\n",
    "    bins_lis.append(i/20)\n",
    "\n",
    "death_lis=df_death_r.loc[\"expbeki_a\"].values\n",
    "death_weight=np.ones_like(death_lis)/len(death_lis)\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.grid(True)\n",
    "plt.hist(death_lis,bins=bins_lis,weights=death_weight,alpha=1)\n",
    "plt.xlabel(r\"$\\beta$\",size=16)\n",
    "plt.ylabel(\"Probability\",size=16)\n",
    "#fig.savefig(f\"C:/Users/naoki/OneDrive/デスクトップ/figure_work/deathbeta.eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#α\n",
    "\n",
    "bins_lis=[]\n",
    "for i in list(range(0,40,1)):\n",
    "    bins_lis.append(i/20)\n",
    "\n",
    "death_lis=df_death_r.loc[\"expbeki_b\"].values#αを抽出\n",
    "death_weight=np.ones_like(death_lis)/len(death_lis)\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.grid(True)\n",
    "plt.hist(death_lis,bins=bins_lis,weights=death_weight,alpha=1)\n",
    "plt.xlabel(r\"$\\alpha$\",size=16)\n",
    "plt.ylabel(\"Probability\",size=16)\n",
    "#fig.savefig(f\"C:/Users/naoki/OneDrive/デスクトップ/figure_work/deathalpha.eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#median\n",
    "print(\"earthquake\")\n",
    "print(df_disaster_r.loc[\"expbeki_a\"].median())\n",
    "print(df_disaster_r.loc[\"expbeki_b\"].median())\n",
    "\n",
    "print(\"notable death\")\n",
    "print(df_death_r.loc[\"expbeki_a\"].median())\n",
    "print(df_death_r.loc[\"expbeki_b\"].median())\n",
    "\n",
    "print(\"aviation accident\")\n",
    "print(df_aviation_r.loc[\"expbeki_a\"].median())\n",
    "print(df_aviation_r.loc[\"expbeki_b\"].median())\n",
    "\n",
    "print(\"mass murder\")\n",
    "print(df_murder_r.loc[\"expbeki_a\"].median())\n",
    "print(df_murder_r.loc[\"expbeki_b\"].median())\n",
    "\n",
    "print(\"tero\")\n",
    "print(df_tero_r.loc[\"expbeki_a\"].median())\n",
    "print(df_tero_r.loc[\"expbeki_b\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve for each parameter (visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_sample=[]\n",
    "n_lis=[]\n",
    "beta_lis=[0,0.25,0.5,0.75,1.0]\n",
    "alpha_lis=[0,0.25,0.5,0.75]\n",
    "alpha_lis.reverse()\n",
    "\n",
    "for alpha in alpha_lis:\n",
    "    for beta in beta_lis:\n",
    "\n",
    "        #条件に当てはまるイベントを一つ取得\n",
    "        name_lis_r=df_death_r.loc[\"expbeki\"][(df_death_r.loc[\"expbeki_a\"]>beta)\n",
    "                                        &(df_death_r.loc[\"expbeki_a\"]<beta+0.25) \n",
    "                                        &(df_death_r.loc[\"expbeki_b\"]>alpha) \n",
    "                                        &(df_death_r.loc[\"expbeki_b\"]<alpha+0.25) \n",
    "                                        &(df_death_r.loc[\"expbeki\"]>0.90)\n",
    "                                        ].index.tolist()\n",
    "        \n",
    "        name_lis=df_death_r.loc[\"expbeki\"][(df_death_r.loc[\"expbeki_a\"]>beta)\n",
    "                                        &(df_death_r.loc[\"expbeki_a\"]<beta+0.25) \n",
    "                                        &(df_death_r.loc[\"expbeki_b\"]>alpha) \n",
    "                                        &(df_death_r.loc[\"expbeki_b\"]<alpha+0.25) \n",
    "                                        ].index.tolist()\n",
    "        \n",
    "        if len([s for s in name_lis if \"%\" not in s])!=0:\n",
    "            name=random.choice([s for s in name_lis if \"%\" not in s])\n",
    "            name_sample.append(name)\n",
    "        else:\n",
    "            name=random.choice(name_lis)\n",
    "            name_sample.append(name)\n",
    "            \n",
    "        n_lis.append(len(name_lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "file=\"death\"\n",
    "df_decay=pd.read_csv(f\"decay_data/nan/{file}_pre.csv\", index_col=0)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "rows_count=4 \n",
    "columns_count=5 \n",
    "graphs_count = rows_count * columns_count \n",
    "axes = [] \n",
    "x = np.linspace(1, 300, 300)\n",
    "\n",
    "for i,name,n in zip(range(1, graphs_count+1),name_sample,n_lis):\n",
    "    \n",
    "    axes.append(fig.add_subplot(rows_count, columns_count, i))\n",
    "    \n",
    "    df_fromax=df_decay[name][df_decay[name].idxmax():]\n",
    "    minimum = np.min(df_fromax.values[np.nonzero(df_fromax.values)])\n",
    "    df=df_fromax+minimum\n",
    "    df_norm=df/df.max()\n",
    "    df_norm.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    y_analysis=np.log10(df_norm)\n",
    "    x_analysis = np.linspace(1,len(y_analysis),len(y_analysis))#x\n",
    "\n",
    "    popt_expbeki, _ = optimize.curve_fit(expbeki, x_analysis, y_analysis, bounds=((0,0,0,0),(np.inf,np.inf,np.inf,np.inf)))\n",
    "    \n",
    "    axes[i-1].scatter(x_analysis,y_analysis,marker='.',s=30, c=\"black\")\n",
    "    axes[i-1].plot(x_analysis,beki(x_analysis,popt_expbeki[2],popt_expbeki[3]),linestyle = \"dashdot\", label = \"power-law\", color = \"red\",alpha = 1)\n",
    "    axes[i-1].plot(x_analysis,exp(x_analysis,popt_expbeki[0],popt_expbeki[1]),linestyle = \"dashed\", label = \"Exponential\", color = \"blue\",alpha = 1)\n",
    "    axes[i-1].plot(x_analysis,expbeki(x_analysis,popt_expbeki[0],popt_expbeki[1],popt_expbeki[2],popt_expbeki[3]),linestyle = \"solid\", label = \"proposed model\", linewidth = 4, color = \"orange\",alpha = 0.7)\n",
    "\n",
    "    axes[i-1].set_ylim(-4,0.2)\n",
    "    axes[i-1].set_title(f'n={n} (ex.{name})', loc='center',size=10)\n",
    "    axes[i-1].set_xscale(\"log\")\n",
    "    axes[i-1].grid(linewidth=0.2)\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.2, hspace=0.4) #グラフ間の隙間調整\n",
    "    \n",
    "plt.show()\n",
    "plt.savefig(f\"C:/Users/naoki/OneDrive/デスクトップ/figure_work/allfitting.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MemorySwitchingPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import math\n",
    "files=[\"disaster\",\"death\",\"aviation\",\"murder\",\"tero\"]\n",
    "\n",
    "for file in files:\n",
    "    df=pd.read_csv(f\"parameter/{file}_log.csv\",index_col=0)\n",
    "    lis=df.loc[\"switching_point\"].values\n",
    "    weight=np.ones_like(lis)/len(lis)\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.xlim(0,60)\n",
    "    bins_lis=list(range(1,61,1))\n",
    "    \n",
    "    plt.hist(lis,bins=bins_lis,weights=weight,alpha=1, histtype=\"stepfilled\")\n",
    "    plt.xlabel(\"Memory switching point $t*$\",size=10)\n",
    "    plt.ylabel(\"Probablity\",size=10)\n",
    "    \n",
    "    if file==\"disaster\":\n",
    "        plt.title(f\"earthquakes(N={len(lis)})\")\n",
    "    elif file==\"tero\":\n",
    "        plt.title(f\"terror(N={len(lis)})\")\n",
    "    else:\n",
    "        plt.title(f\"{file}(N={len(lis)})\")\n",
    "    \n",
    "    plt.show()\n",
    "    fig.savefig(f\"C:/Users/naoki/OneDrive/デスクトップ/figure_work/{file}_koten.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
